{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11286702,"sourceType":"datasetVersion","datasetId":7056964}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        file_path = os.path.join(dirname, filename)\n        print(file_path)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:27:33.439549Z","iopub.execute_input":"2026-01-03T15:27:33.440245Z","iopub.status.idle":"2026-01-03T15:27:33.447051Z","shell.execute_reply.started":"2026-01-03T15:27:33.440215Z","shell.execute_reply":"2026-01-03T15:27:33.446428Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/legle-dataset/combined_content.txt\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport math\nimport numpy as np\nfrom collections import Counter\nimport re\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:27:33.448296Z","iopub.execute_input":"2026-01-03T15:27:33.448537Z","iopub.status.idle":"2026-01-03T15:27:33.456548Z","shell.execute_reply.started":"2026-01-03T15:27:33.448517Z","shell.execute_reply":"2026-01-03T15:27:33.455914Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class LegalTextTokenizer:\n    \"\"\"Tokenizer for legal text with vocabulary management\"\"\"\n    \n    def __init__(self, vocab_size=15000):\n        self.vocab_size = vocab_size\n        self.word2idx = {}\n        self.idx2word = {}\n        \n    def build_vocab(self, text):\n        \"\"\"Build vocabulary from text\"\"\"\n        # Clean and tokenize\n        text = text.lower()\n        text = re.sub(r'\\s+', ' ', text)\n        words = text.split()\n        \n        # Count word frequencies\n        word_counts = Counter(words)\n        most_common = word_counts.most_common(self.vocab_size - 4)\n        \n        # Special tokens\n        self.word2idx = {\n            '<PAD>': 0,\n            '<UNK>': 1,\n            '<SOS>': 2,  # Start of sequence\n            '<EOS>': 3   # End of sequence\n        }\n        \n        # Add words to vocabulary\n        for idx, (word, _) in enumerate(most_common, start=4):\n            self.word2idx[word] = idx\n            \n        self.idx2word = {v: k for k, v in self.word2idx.items()}\n        print(f\"✓ Vocabulary built: {len(self.word2idx)} tokens\")\n        \n    def encode(self, text):\n        \"\"\"Convert text to token IDs\"\"\"\n        words = text.lower().split()\n        return [self.word2idx.get(w, self.word2idx['<UNK>']) for w in words]\n    \n    def decode(self, token_ids):\n        \"\"\"Convert token IDs to text\"\"\"\n        return ' '.join([self.idx2word.get(idx, '<UNK>') for idx in token_ids])\n    \n    def save(self, path):\n        \"\"\"Save tokenizer\"\"\"\n        torch.save({\n            'word2idx': self.word2idx,\n            'idx2word': self.idx2word,\n            'vocab_size': self.vocab_size\n        }, path)\n        \n    def load(self, path):\n        \"\"\"Load tokenizer\"\"\"\n        data = torch.load(path)\n        self.word2idx = data['word2idx']\n        self.idx2word = data['idx2word']\n        self.vocab_size = data['vocab_size']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:27:33.457709Z","iopub.execute_input":"2026-01-03T15:27:33.458273Z","iopub.status.idle":"2026-01-03T15:27:33.474382Z","shell.execute_reply.started":"2026-01-03T15:27:33.458242Z","shell.execute_reply":"2026-01-03T15:27:33.473676Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class CausalLMDataset(Dataset):\n    \"\"\"Dataset for next-token prediction with causal masking\"\"\"\n    \n    def __init__(self, token_ids, seq_length=128):\n        self.seq_length = seq_length\n        self.data = []\n        \n        # Create sequences\n        for i in range(0, len(token_ids) - seq_length - 1, seq_length // 2):\n            input_seq = token_ids[i:i + seq_length]\n            target_seq = token_ids[i + 1:i + seq_length + 1]\n            \n            if len(input_seq) == seq_length and len(target_seq) == seq_length:\n                self.data.append((input_seq, target_seq))\n        \n        print(f\"✓ Created {len(self.data)} training sequences\")\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return (\n            torch.tensor(self.data[idx][0], dtype=torch.long),\n            torch.tensor(self.data[idx][1], dtype=torch.long)\n        )\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:27:33.475085Z","iopub.execute_input":"2026-01-03T15:27:33.475349Z","iopub.status.idle":"2026-01-03T15:27:33.489530Z","shell.execute_reply.started":"2026-01-03T15:27:33.475330Z","shell.execute_reply":"2026-01-03T15:27:33.488915Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    \"\"\"\n    Multi-Head Self-Attention Mechanism\n    WHY: Allows model to attend to different aspects of input simultaneously\n    \"\"\"\n    \n    def __init__(self, d_model, num_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads  # Dimension per head\n        \n        # Linear projections for Q, K, V\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask=None):\n        batch_size, seq_len, d_model = x.size()\n        \n        # Linear projections and reshape for multi-head\n        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n        \n        # Scaled dot-product attention\n        # WHY: Scaled by sqrt(d_k) to prevent softmax saturation\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        # Apply causal mask (prevent attending to future tokens)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n        \n        # Attention weights\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n        \n        # Apply attention to values\n        context = torch.matmul(attn_weights, V)\n        \n        # Concatenate heads and apply output projection\n        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n        output = self.W_o(context)\n        \n        return output\n\n\n# ============================================================================\n# Step 4: Feed-Forward Network\n# ============================================================================\n\nclass FeedForward(nn.Module):\n    \"\"\"\n    Position-wise Feed-Forward Network\n    WHY: Adds non-linearity and processes each position independently\n    \"\"\"\n    \n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        # WHY GELU: Smooth activation, works better than ReLU for transformers\n        return self.linear2(self.dropout(F.gelu(self.linear1(x))))\n\n\n# ============================================================================\n# Step 5: Decoder Block (Transformer Layer)\n# ============================================================================\n\nclass DecoderBlock(nn.Module):\n    \"\"\"\n    Single Transformer Decoder Block\n    WHY: Stacking these creates deep understanding of text\n    \"\"\"\n    \n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        \n        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        \n        # Layer normalization\n        # WHY: Stabilizes training in deep networks\n        self.ln1 = nn.LayerNorm(d_model)\n        self.ln2 = nn.LayerNorm(d_model)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask):\n        # Multi-head attention with residual connection\n        attn_output = self.attention(self.ln1(x), mask)\n        x = x + self.dropout(attn_output)\n        \n        # Feed-forward with residual connection\n        ff_output = self.feed_forward(self.ln2(x))\n        x = x + self.dropout(ff_output)\n        \n        return x\n\n\n# ============================================================================\n# Step 6: Positional Encoding\n# ============================================================================\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"\n    Sinusoidal Positional Encoding\n    WHY: Injects position information (transformers don't know word order)\n    \"\"\"\n    \n    def __init__(self, d_model, max_len=5000, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        \n        # Create positional encoding matrix\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                            (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        self.register_buffer('pe', pe.unsqueeze(0))\n        \n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)\n\n\n# ============================================================================\n# Step 7: Full Decoder-Based LLM (GPT-style)\n# ============================================================================\n\nclass DecoderLLM(nn.Module):\n    \"\"\"\n    Full Decoder-Only Large Language Model\n    Architecture: GPT-style with causal multi-head attention\n    \"\"\"\n    \n    def __init__(self, vocab_size, d_model=512, num_heads=8, num_layers=6, \n                 d_ff=2048, max_seq_len=512, dropout=0.1):\n        super().__init__()\n        \n        self.d_model = d_model\n        self.num_layers = num_layers\n        \n        # Token embedding\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n        print(\"embedding : \",self.token_embedding)\n        # Positional encoding\n        self.pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n        \n        # Stack of decoder blocks\n        self.decoder_blocks = nn.ModuleList([\n            DecoderBlock(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_layers)\n        ])\n        \n        # Final layer norm\n        self.ln_f = nn.LayerNorm(d_model)\n        \n        # Output projection to vocabulary\n        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n        \n        # Tie weights (share embeddings with output layer)\n        # WHY: Reduces parameters and improves generalization\n        self.lm_head.weight = self.token_embedding.weight\n        \n        # Initialize weights\n        self.apply(self._init_weights)\n        \n    def _init_weights(self, module):\n        \"\"\"Initialize weights with small random values\"\"\"\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n    \n    def forward(self, x):\n        batch_size, seq_len = x.size()\n        \n        # Create causal mask (lower triangular)\n        # WHY: Prevents attending to future tokens\n        mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)\n        mask = mask.to(x.device)\n        \n        # Token embeddings + positional encoding\n        x = self.token_embedding(x) * math.sqrt(self.d_model)\n        x = self.pos_encoding(x)\n        \n        # Pass through decoder blocks\n        for decoder_block in self.decoder_blocks:\n            x = decoder_block(x, mask)\n        \n        # Final layer norm\n        x = self.ln_f(x)\n        \n        # Project to vocabulary\n        logits = self.lm_head(x)\n        \n        return logits\n    \n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Generate text autoregressively\n        WHY: Sample from probability distribution to create diverse outputs\n        \"\"\"\n        for _ in range(max_new_tokens):\n            # Crop context if too long\n            idx_cond = idx if idx.size(1) <= 512 else idx[:, -512:]\n            \n            # Forward pass\n            logits = self(idx_cond)\n            logits = logits[:, -1, :] / temperature\n            \n            # Optional top-k sampling\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            \n            # Sample from distribution\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            \n            # Append to sequence\n            idx = torch.cat((idx, idx_next), dim=1)\n        \n        return idx\n\n\n# ============================================================================\n# Step 8: Training Function\n# ============================================================================\n\ndef train_model(model, train_loader, epochs=10, lr=3e-4, device='cuda'):\n    \"\"\"Train the decoder-based LLM\"\"\"\n    \n    model = model.to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95), \n                                   weight_decay=0.1)\n    \n    # WHY: Cosine learning rate schedule - smooth decay improves convergence\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n    \n    model.train()\n    \n    for epoch in range(epochs):\n        total_loss = 0\n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # Forward pass\n            logits = model(inputs)\n            \n            # Reshape for loss calculation\n            # WHY: CrossEntropyLoss expects (batch*seq, vocab_size)\n            loss = F.cross_entropy(\n                logits.view(-1, logits.size(-1)),\n                targets.view(-1)\n            )\n            \n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            \n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            \n            optimizer.step()\n            \n            total_loss += loss.item()\n            \n            if batch_idx % 50 == 0:\n                print(f'Epoch [{epoch+1}/{epochs}] Batch [{batch_idx}/{len(train_loader)}] '\n                      f'Loss: {loss.item():.4f} LR: {scheduler.get_last_lr()[0]:.6f}')\n        \n        avg_loss = total_loss / len(train_loader)\n        print(f'\\n{\"=\"*70}')\n        print(f'Epoch {epoch+1} Complete - Avg Loss: {avg_loss:.4f}')\n        print(f'{\"=\"*70}\\n')\n        \n        scheduler.step()\n    \n    return model\n\n\n# ============================================================================\n# Step 9: Text Generation Function\n# ============================================================================\n\ndef generate_legal_text(model, tokenizer, prompt, max_tokens=100, \n                       temperature=0.8, top_k=50, device='cuda'):\n    \"\"\"Generate text from prompt\"\"\"\n    \n    model.eval()\n    model = model.to(device)\n    \n    # Encode prompt\n    input_ids = tokenizer.encode(prompt)\n    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n    \n    # Generate\n    with torch.no_grad():\n        output_ids = model.generate(\n            input_tensor,\n            max_new_tokens=max_tokens,\n            temperature=temperature,\n            top_k=top_k\n        )\n    \n    # Decode\n    generated_text = tokenizer.decode(output_ids[0].tolist())\n    \n    return generated_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:27:33.581314Z","iopub.execute_input":"2026-01-03T15:27:33.581549Z","iopub.status.idle":"2026-01-03T15:27:33.608513Z","shell.execute_reply.started":"2026-01-03T15:27:33.581527Z","shell.execute_reply":"2026-01-03T15:27:33.607820Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*70)\nprint(\"DECODER-BASED LARGE LANGUAGE MODEL FOR LEGAL TEXT\")\nprint(\"=\"*70 + \"\\n\")\n    \n# Configuration\nCONFIG = {\n        'text_file': file_path,\n        'vocab_size': 15000,\n        'seq_length': 512,\n        'batch_size': 32,\n        'd_model': 512,\n        'num_heads': 8,\n        'num_layers': 6,\n        'd_ff': 2048,\n        'dropout': 0.1,\n        'epochs':3,\n        'learning_rate': 3e-4,\n    }\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\\n\")\n    \n# Step 1: Load and tokenize text\nprint(\"STEP 1: Loading and tokenizing text...\")\nwith open(CONFIG['text_file'], 'r', encoding='utf-8') as f:\n     text = f.read()\n\nprint(text[:200])\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:27:33.609784Z","iopub.execute_input":"2026-01-03T15:27:33.609978Z","iopub.status.idle":"2026-01-03T15:27:34.464019Z","shell.execute_reply.started":"2026-01-03T15:27:33.609958Z","shell.execute_reply":"2026-01-03T15:27:34.463412Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nDECODER-BASED LARGE LANGUAGE MODEL FOR LEGAL TEXT\n======================================================================\n\nDevice: cuda\n\nSTEP 1: Loading and tokenizing text...\n\"The Andaman and Nicobar Islands (Municipal) Corporation \nRegulation,1994  \n(Creation of The Andaman and Nicobar Islands (Municipal) Corporation \nRegulation,1994 is in progress and shall be uploaded s\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"tokenizer = LegalTextTokenizer(vocab_size=CONFIG['vocab_size'])\ntokenizer.build_vocab(text)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:27:34.464816Z","iopub.execute_input":"2026-01-03T15:27:34.465063Z","iopub.status.idle":"2026-01-03T15:27:39.574264Z","shell.execute_reply.started":"2026-01-03T15:27:34.465031Z","shell.execute_reply":"2026-01-03T15:27:39.573511Z"}},"outputs":[{"name":"stdout","text":"✓ Vocabulary built: 15000 tokens\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# tokenizer.word2idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:27:39.575249Z","iopub.execute_input":"2026-01-03T15:27:39.575590Z","iopub.status.idle":"2026-01-03T15:27:39.578816Z","shell.execute_reply.started":"2026-01-03T15:27:39.575567Z","shell.execute_reply":"2026-01-03T15:27:39.578231Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Step 2: Create dataset\nprint(\"\\nSTEP 2: Creating dataset...\")\ntoken_ids = tokenizer.encode(text)\ntoken_ids[0:10]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:27:39.580438Z","iopub.execute_input":"2026-01-03T15:27:39.580810Z","iopub.status.idle":"2026-01-03T15:27:41.447235Z","shell.execute_reply.started":"2026-01-03T15:27:39.580778Z","shell.execute_reply":"2026-01-03T15:27:41.446581Z"}},"outputs":[{"name":"stdout","text":"\nSTEP 2: Creating dataset...\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[44, 6506, 9, 6507, 6897, 1, 119, 1, 6704, 5]"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"dataset = CausalLMDataset(token_ids, seq_length=CONFIG['seq_length'])\n# dataset.data[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:27:41.448069Z","iopub.execute_input":"2026-01-03T15:27:41.448352Z","iopub.status.idle":"2026-01-03T15:27:42.130480Z","shell.execute_reply.started":"2026-01-03T15:27:41.448331Z","shell.execute_reply":"2026-01-03T15:27:42.129699Z"}},"outputs":[{"name":"stdout","text":"✓ Created 27815 training sequences\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"train_loader = DataLoader(dataset, batch_size=CONFIG['batch_size'], \n                             shuffle=True, num_workers=2, pin_memory=True)\ntrain_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:27:42.131481Z","iopub.execute_input":"2026-01-03T15:27:42.131868Z","iopub.status.idle":"2026-01-03T15:27:42.136846Z","shell.execute_reply.started":"2026-01-03T15:27:42.131838Z","shell.execute_reply":"2026-01-03T15:27:42.136170Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<torch.utils.data.dataloader.DataLoader at 0x79a9347f21e0>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Step 3: Initialize model\nprint(\"\\nSTEP 3: Initializing Decoder-Based LLM...\")\nmodel = DecoderLLM(\n        vocab_size=len(tokenizer.word2idx),\n        d_model=CONFIG['d_model'],\n        num_heads=CONFIG['num_heads'],\n        num_layers=CONFIG['num_layers'],\n        d_ff=CONFIG['d_ff'],\n        max_seq_len=CONFIG['seq_length'],\n        dropout=CONFIG['dropout']\n    )\n    \ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"✓ Model initialized\")\nprint(f\"  Total parameters: {total_params:,}\")\nprint(f\"  Trainable parameters: {trainable_params:,}\")\nprint(f\"  Model size: ~{total_params * 4 / 1e6:.2f} MB\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:27:42.137766Z","iopub.execute_input":"2026-01-03T15:27:42.138030Z","iopub.status.idle":"2026-01-03T15:27:42.671873Z","shell.execute_reply.started":"2026-01-03T15:27:42.138001Z","shell.execute_reply":"2026-01-03T15:27:42.671100Z"}},"outputs":[{"name":"stdout","text":"\nSTEP 3: Initializing Decoder-Based LLM...\nembedding :  Embedding(15000, 512)\n✓ Model initialized\n  Total parameters: 26,595,328\n  Trainable parameters: 26,595,328\n  Model size: ~106.38 MB\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"    \n# Step 4: Train model\nprint(\"\\n\" + \"=\"*70)\nprint(\"STEP 4: Training Model\")\nprint(\"=\"*70 + \"\\n\")\n    \nmodel = train_model(\n        model, \n        train_loader, \n        epochs=CONFIG['epochs'],\n        lr=CONFIG['learning_rate'],\n        device=device\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:27:42.672884Z","iopub.execute_input":"2026-01-03T15:27:42.673281Z","iopub.status.idle":"2026-01-03T15:51:27.662289Z","shell.execute_reply.started":"2026-01-03T15:27:42.673256Z","shell.execute_reply":"2026-01-03T15:51:27.661462Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nSTEP 4: Training Model\n======================================================================\n\nEpoch [1/3] Batch [0/870] Loss: 9.7031 LR: 0.000300\nEpoch [1/3] Batch [50/870] Loss: 5.4869 LR: 0.000300\nEpoch [1/3] Batch [100/870] Loss: 4.5218 LR: 0.000300\nEpoch [1/3] Batch [150/870] Loss: 4.1795 LR: 0.000300\nEpoch [1/3] Batch [200/870] Loss: 3.7943 LR: 0.000300\nEpoch [1/3] Batch [250/870] Loss: 3.7070 LR: 0.000300\nEpoch [1/3] Batch [300/870] Loss: 3.4357 LR: 0.000300\nEpoch [1/3] Batch [350/870] Loss: 3.0726 LR: 0.000300\nEpoch [1/3] Batch [400/870] Loss: 2.8745 LR: 0.000300\nEpoch [1/3] Batch [450/870] Loss: 2.8182 LR: 0.000300\nEpoch [1/3] Batch [500/870] Loss: 2.5637 LR: 0.000300\nEpoch [1/3] Batch [550/870] Loss: 2.3144 LR: 0.000300\nEpoch [1/3] Batch [600/870] Loss: 2.1931 LR: 0.000300\nEpoch [1/3] Batch [650/870] Loss: 1.8758 LR: 0.000300\nEpoch [1/3] Batch [700/870] Loss: 1.8705 LR: 0.000300\nEpoch [1/3] Batch [750/870] Loss: 1.8676 LR: 0.000300\nEpoch [1/3] Batch [800/870] Loss: 1.4194 LR: 0.000300\nEpoch [1/3] Batch [850/870] Loss: 1.5851 LR: 0.000300\n\n======================================================================\nEpoch 1 Complete - Avg Loss: 3.0433\n======================================================================\n\nEpoch [2/3] Batch [0/870] Loss: 1.4022 LR: 0.000225\nEpoch [2/3] Batch [50/870] Loss: 1.1181 LR: 0.000225\nEpoch [2/3] Batch [100/870] Loss: 1.2990 LR: 0.000225\nEpoch [2/3] Batch [150/870] Loss: 1.3393 LR: 0.000225\nEpoch [2/3] Batch [200/870] Loss: 1.1539 LR: 0.000225\nEpoch [2/3] Batch [250/870] Loss: 0.9386 LR: 0.000225\nEpoch [2/3] Batch [300/870] Loss: 0.7593 LR: 0.000225\nEpoch [2/3] Batch [350/870] Loss: 0.9284 LR: 0.000225\nEpoch [2/3] Batch [400/870] Loss: 0.9192 LR: 0.000225\nEpoch [2/3] Batch [450/870] Loss: 0.9947 LR: 0.000225\nEpoch [2/3] Batch [500/870] Loss: 0.8187 LR: 0.000225\nEpoch [2/3] Batch [550/870] Loss: 0.6655 LR: 0.000225\nEpoch [2/3] Batch [600/870] Loss: 0.5167 LR: 0.000225\nEpoch [2/3] Batch [650/870] Loss: 0.8235 LR: 0.000225\nEpoch [2/3] Batch [700/870] Loss: 0.5924 LR: 0.000225\nEpoch [2/3] Batch [750/870] Loss: 0.4900 LR: 0.000225\nEpoch [2/3] Batch [800/870] Loss: 0.5391 LR: 0.000225\nEpoch [2/3] Batch [850/870] Loss: 0.9717 LR: 0.000225\n\n======================================================================\nEpoch 2 Complete - Avg Loss: 0.9029\n======================================================================\n\nEpoch [3/3] Batch [0/870] Loss: 0.5584 LR: 0.000075\nEpoch [3/3] Batch [50/870] Loss: 0.5138 LR: 0.000075\nEpoch [3/3] Batch [100/870] Loss: 0.5711 LR: 0.000075\nEpoch [3/3] Batch [150/870] Loss: 0.4185 LR: 0.000075\nEpoch [3/3] Batch [200/870] Loss: 0.8090 LR: 0.000075\nEpoch [3/3] Batch [250/870] Loss: 0.2881 LR: 0.000075\nEpoch [3/3] Batch [300/870] Loss: 0.3150 LR: 0.000075\nEpoch [3/3] Batch [350/870] Loss: 0.4182 LR: 0.000075\nEpoch [3/3] Batch [400/870] Loss: 0.4497 LR: 0.000075\nEpoch [3/3] Batch [450/870] Loss: 0.7053 LR: 0.000075\nEpoch [3/3] Batch [500/870] Loss: 0.3393 LR: 0.000075\nEpoch [3/3] Batch [550/870] Loss: 0.4261 LR: 0.000075\nEpoch [3/3] Batch [600/870] Loss: 0.4185 LR: 0.000075\nEpoch [3/3] Batch [650/870] Loss: 0.6911 LR: 0.000075\nEpoch [3/3] Batch [700/870] Loss: 0.2845 LR: 0.000075\nEpoch [3/3] Batch [750/870] Loss: 0.3085 LR: 0.000075\nEpoch [3/3] Batch [800/870] Loss: 0.3443 LR: 0.000075\nEpoch [3/3] Batch [850/870] Loss: 0.4380 LR: 0.000075\n\n======================================================================\nEpoch 3 Complete - Avg Loss: 0.4936\n======================================================================\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"prompt = \"\"\"The Andaman and Nicobar Islands (Municipal) Corporation \nRegulation,1994\"\"\"\ngenerated = generate_legal_text(\n            model, tokenizer, prompt, \n            max_tokens=50, temperature=0.7, top_k=50, device=device)\nprint(f\"Generated: {generated}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:51:27.663800Z","iopub.execute_input":"2026-01-03T15:51:27.664324Z","iopub.status.idle":"2026-01-03T15:51:28.168271Z","shell.execute_reply.started":"2026-01-03T15:51:27.664295Z","shell.execute_reply":"2026-01-03T15:51:28.167527Z"}},"outputs":[{"name":"stdout","text":"Generated: the andaman and nicobar islands <UNK> corporation <UNK> (2) for the purpose of this section shall have the same meaning as in respect of the act or the rules made thereunder to it for the purpose of this act. (3) every rule made under this section shall be laid as soon as may be after it is made,\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Step 6: Test generation\nprint(\"\\n\" + \"=\"*70)\nprint(\"STEP 6: Testing Text Generation\")\nprint(\"=\"*70 + \"\\n\")\n    \ntest_prompts = [\n        \"section 2 of the indian penal code\",\n        \"according to the constitution of india\",\n        \"the supreme court has ruled that\"]\n    \nfor prompt in test_prompts:\n    print(f\"\\nPrompt: '{prompt}'\")\n    print(\"-\" * 70)\n    generated = generate_legal_text(\n            model, tokenizer, prompt, \n            max_tokens=50, temperature=0.7, top_k=50, device=device)\n    print(f\"Generated: {generated}\")\n    print()\n    \nprint(\"=\"*70)\nprint(\"TRAINING AND TESTING COMPLETE!\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:51:28.169287Z","iopub.execute_input":"2026-01-03T15:51:28.169644Z","iopub.status.idle":"2026-01-03T15:51:28.839574Z","shell.execute_reply.started":"2026-01-03T15:51:28.169621Z","shell.execute_reply":"2026-01-03T15:51:28.838952Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nSTEP 6: Testing Text Generation\n======================================================================\n\n\nPrompt: 'section 2 of the indian penal code'\n----------------------------------------------------------------------\nGenerated: section 2 of the indian penal code (45 of 1860). (4) the provisions of the code of criminal procedure, 1973 (2 of 1974) shall, so far as may be, apply to any search or seizure under this section as they apply to any search or seizure made under the authority of a warrant issued under 99 [section\n\n\nPrompt: 'according to the constitution of india'\n----------------------------------------------------------------------\nGenerated: according to the constitution of india and the requirements of the situation. (3) the recommendations of the advisory committee shall be advisory in nature.\" \"the document **tn_the_building_and_other_construction_workers_reg.pdf** provides essential regulations and guidelines. this section, **41e emergency standards**, covers specific details relevant to its scope. 41-e. emergency standards (1) where the central government is satisfied that no\n\n\nPrompt: 'the supreme court has ruled that'\n----------------------------------------------------------------------\nGenerated: the supreme court has <UNK> that employer shall be entitled to receive wages for the period of not less than six months during the preceding the date of the award and the wage period shall be limited to wages by such <UNK> \"the document **c_the_mines_act_1952.pdf** provides essential regulations and guidelines. this section, <UNK> covers specific details\n\n======================================================================\nTRAINING AND TESTING COMPLETE!\n======================================================================\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"    # Step 5: Save model\nprint(\"\\nSTEP 5: Saving model and tokenizer...\")\nos.makedirs('models', exist_ok=True)\n    \ntorch.save({\n        'model_state_dict': model.state_dict(),\n        'config': CONFIG,\n        'vocab_size': len(tokenizer.word2idx)\n    }, 'models/legal_llm.pt')\n    \ntokenizer.save('models/tokenizer.pt')\nprint(\"✓ Model saved to 'models/legal_llm.pt'\")\nprint(\"✓ Tokenizer saved to 'models/tokenizer.pt'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:51:28.840561Z","iopub.execute_input":"2026-01-03T15:51:28.840945Z","iopub.status.idle":"2026-01-03T15:51:29.037605Z","shell.execute_reply.started":"2026-01-03T15:51:28.840921Z","shell.execute_reply":"2026-01-03T15:51:29.036925Z"}},"outputs":[{"name":"stdout","text":"\nSTEP 5: Saving model and tokenizer...\n✓ Model saved to 'models/legal_llm.pt'\n✓ Tokenizer saved to 'models/tokenizer.pt'\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"def load_trained_model(model_path='models/legal_llm.pt', \n                      tokenizer_path='models/tokenizer.pt'):\n    \"\"\"Load a trained model for inference\"\"\"\n    \n    # Load checkpoint\n    checkpoint = torch.load(model_path)\n    config = checkpoint['config']\n    \n    # Load tokenizer\n    tokenizer = LegalTextTokenizer()\n    tokenizer.load(tokenizer_path)\n    \n    # Initialize model\n    model = DecoderLLM(\n        vocab_size=checkpoint['vocab_size'],\n        d_model=config['d_model'],\n        num_heads=config['num_heads'],\n        num_layers=config['num_layers'],\n        d_ff=config['d_ff'],\n        dropout=config['dropout']\n    )\n    \n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n    \n    print(\"✓ Model and tokenizer loaded successfully!\")\n    \n    return model, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:52:47.325101Z","iopub.execute_input":"2026-01-03T15:52:47.325638Z","iopub.status.idle":"2026-01-03T15:52:47.330732Z","shell.execute_reply.started":"2026-01-03T15:52:47.325609Z","shell.execute_reply":"2026-01-03T15:52:47.329980Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"  \n    # After training, use the model:\n    model, tokenizer = load_trained_model()\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:53:01.521380Z","iopub.execute_input":"2026-01-03T15:53:01.521680Z","iopub.status.idle":"2026-01-03T15:53:02.186268Z","shell.execute_reply.started":"2026-01-03T15:53:01.521652Z","shell.execute_reply":"2026-01-03T15:53:02.185589Z"}},"outputs":[{"name":"stdout","text":"embedding :  Embedding(15000, 512)\n✓ Model and tokenizer loaded successfully!\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"text = generate_legal_text(model, tokenizer, \"What are the provisions regarding working hours for employees\")\ntext","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:54:35.661057Z","iopub.execute_input":"2026-01-03T15:54:35.661662Z","iopub.status.idle":"2026-01-03T15:54:36.127817Z","shell.execute_reply.started":"2026-01-03T15:54:35.661636Z","shell.execute_reply":"2026-01-03T15:54:36.127190Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"'what are the provisions regarding working hours for employees and such conditions as it may deem necessary. 57. returns every principal employer shall maintain accurate and up-to-date health hazards within the local limits of the factory or establishment in the factory or the principal inspectors whose local are otherwise than one thousand or more inspectors shall respectively exercise the powers and perform such duties as may be specified in the order, the corporation or may specify in the vicinity of the factory or establishment in the purpose of securing the health and safety of securing the workers employed in the factory or establishment in connection with the health of'"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}